# 🖼️ Deep Learning and its Applications to Signal and Image Processing and Analysis S2-2025  
**Course:** 361-2-1120  
**Submission Date:** 27.06.2025  
**Final Project:** Super-Resolution Image Enhancement  

**Students:**  
- Tomer Abram (208931691)  
- Laith Sadik (318679677)  

## 📌 Project Overview

## Dataset
We use the **DIV2K** dataset with 900 high-quality images (800 train, 100 validation).  
- HR images are 2040×1400 pixels approx.  
- LR images are generated by bicubic downsampling ×4.  
- Training uses paired patches: HR (96×96) and LR (24×24).
- 
This project focuses on solving the **Single Image Super-Resolution (SISR)** problem: generating high-resolution images from low-resolution inputs using deep neural networks. Two models are compared:
- **Baseline**: SRResNet – a well-known architecture based on residual learning.
- **Our Model**: A Super-Resolution model built completely from scratch, It's somewhat similar as EDSR. It features deeper residual blocks without BatchNorm for better detail preservation, adaptive learning rate scheduling, and improved normalization layers. This model is both a ground-up implementation and an enhancement over the original EDSR architecture.


We evaluated performance using **PSNR**, **SSIM**, and **FID**, and included an **ablation study** replacing L1 loss with MSE to assess the effect of the loss function.


## 🚀 How to Run
The entire code is contained in a single Jupyter Notebook:  
**`TA-208931691_LS-318679677.ipynb`**

### ✅ Requirements:
- Python 3.8+
- Google Colab or local Jupyter
- PyTorch
- torchvision
- numpy, matplotlib, tqdm, etc.

### 📂 Running via Google Colab:
1. **Upload the notebook to your Google Drive.**
2. **Open it in Google Colab.**
3. The dataset (DIV2K) is loaded using Kaggle API or pre-uploaded patches.
4. Press **Run All** – the notebook includes training both SRResNet and our custom model, as well as evaluation, ablation, and visualizations.

### 📌 Notes:
- Training is done on ×4 downscaled images from DIV2K.
- Results include **PSNR/SSIM/FID**, side-by-side visual comparisons, and loss curves.
- Checkpoints:The model checkpoint files are saved on Google Drive:  
[Google Drive Checkpoints Folder](https://drive.google.com/drive/folders/1qV4DR6QplNjXVDiWd5TEd2OnCCW9H9gG?usp=sharing)

## 📊 Evaluation Metrics
We evaluate using:
- **PSNR (Peak Signal-to-Noise Ratio)** – pixel-wise accuracy.
- **SSIM (Structural Similarity Index)** – perceptual similarity.
- **FID (Fréchet Inception Distance)** – realism of generated images.

## 🔍 Ablation Study
We compare:
- **L1 Loss (baseline)** – better at preserving details.  
- **MSE Loss (ablation)** – faster convergence but slightly worse perceptual quality.  

We show that L1 outperforms MSE in final results despite slower convergence.

## 📷 Sample Results
- Examples where both models succeed/fail
- Case studies where our custom model outperforms SRResNet.
- Visual examples from validation set crops.
- Quantitative improvements in PSNR, SSIM, and FID (decreasing over epochs).
- Visual examples show reduced blur and sharper textures on super-resolved images.
- Model generalizes well from patches to full images during inference.
